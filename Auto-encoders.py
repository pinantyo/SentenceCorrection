# -*- coding: utf-8 -*-
"""Lightweight Style Text Transfer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18mrPt0VujooIuuHaE15MvxqJ2JCKDC3k

# Referensi:
* https://towardsdatascience.com/how-to-implement-seq2seq-lstm-model-in-keras-shortcutnlp-6f355f3e5639
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install -q tensorflow-text wandb
# %pip install --upgrade -q tensorflow

import pandas as pd
import numpy as np

import os, re

import tensorflow as tf
import tensorflow_text as tf_text

import matplotlib.pyplot as plt
import matplotlib.ticker as ticker

import wandb

"""# Wandb Initialization"""

wandb.login()

wandb.init(project="textstyletransfer", resume=False)

"""# Data - Processing"""

df_30 = pd.read_csv(
    "/content/drive/MyDrive/PA_NLP/Torche Intern NLP Progress/Progress/datasets/Indonesian Augmented Dataset-30.csv",
    encoding="utf-8"
)

df_40 = pd.read_csv(
    "/content/drive/MyDrive/PA_NLP/Torche Intern NLP Progress/Progress/datasets/Indonesian Augmented Dataset-40.csv",
    encoding="utf-8"
)

df_50 = pd.read_csv(
    "/content/drive/MyDrive/PA_NLP/Torche Intern NLP Progress/Progress/datasets/Indonesian Augmented Dataset-50.csv",
    encoding="utf-8"
)

df = pd.concat([
    df_30,
    df_40,
    df_50],
    ignore_index=True
)

df.info()

"""# Processing"""

df.shape

duplicate = df.duplicated().sum()
if duplicate:
  df.drop_duplicates(
      subset=['Kalimat'],
      inplace=True,
      ignore_index=True
  )

df['Formal Sentences'] = df['Formal Sentences'].astype('str')
df['Kalimat'] = df['Kalimat'].astype('str')

datasets = tf.data.Dataset.from_tensor_slices((df['Kalimat'].values, df['Formal Sentences'].values))

def standardize(text):
    # Split accecented characters.
    text = tf_text.normalize_utf8(text, 'NFKD')
    text = tf.strings.lower(text)
    # Keep space, a to z, and select punctuation.
    text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')
    # Add spaces around punctuation.
    text = tf.strings.regex_replace(text, '[.?!,¿]', r' \0 ')
    # Strip whitespace.
    text = tf.strings.strip(text)

    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')
    return text
  
target_text_processor = tf.keras.layers.TextVectorization(standardize=standardize, max_tokens=5000)
raw_text_processor = tf.keras.layers.TextVectorization(standardize=standardize, max_tokens=5000)

target_text_ds = datasets.map(lambda x, y: y)
raw_text_ds = datasets.map(lambda x, y: x)

BATCH_SIZE = 64


target_text_processor.adapt(target_text_ds.batch(BATCH_SIZE))
raw_text_processor.adapt(raw_text_ds.batch(BATCH_SIZE))

"""# Modelling

## Seq2Seq Bidrectional LSTM
"""

class NeuralTextTransfer(tf.keras.Model):
  def __init__(self, original, target, units=512):
    super().__init__()
    
    self.original_process_text = original
    self.target_process_text = target

    self.ori_vocab_size = len(original.get_vocabulary())
    self.tgt_vocab_size = len(target.get_vocabulary())


    # Encoder Embeddings
    self.ori_embedding = tf.keras.layers.Embedding(
        self.ori_vocab_size,
        output_dim=units,
        mask_zero=True
    )


    self.encoder = tf.keras.layers.Bidirectional(
        layer = tf.keras.layers.LSTM(
            int(units/2),
            return_sequences=True,
            return_state=True
        ),
    )

    self.encoder2 = tf.keras.layers.Bidirectional(
        layer = tf.keras.layers.LSTM(
            int(units/2),
            return_sequences=True,
            return_state=True
        ),
    )

    # Decoder Embeddings
    self.tgt_embedding = tf.keras.layers.Embedding(
        self.tgt_vocab_size,
        output_dim=units,
        mask_zero=True
    )

    self.decoder = tf.keras.layers.LSTM(
        int(units),
        return_sequences=True,
        return_state=True
    )

    self.decoder2 = tf.keras.layers.LSTM(
        int(units),
        return_sequences=True,
        return_state=True
    )

    # Attention
    self.attention = tf.keras.layers.Attention()

    # Output
    self.out = tf.keras.layers.Dense(self.tgt_vocab_size)
  
  def call(self, original_txt, target_txt):
    # Encoder
    ori_tokens = self.original_process_text(original_txt)
    ori_vectors = self.ori_embedding(ori_tokens)

    ori_model, forward_h, forward_c, backward_h, backward_c = self.encoder2(
        self.encoder(ori_vectors)
    )
    
    ori_state_h = tf.concat([forward_h, backward_h], -1)
    ori_state_c = tf.concat([forward_c, backward_c], -1)

    # Decoder
    tgt_tokens = self.target_process_text(target_txt)
    expected = tgt_tokens[:, 1:]

    teacher_forcing = tgt_tokens[:, :-1]
    tgt_vectors = self.tgt_embedding(teacher_forcing)


    tgt_model_attention = self.attention(
        inputs=[
            tgt_vectors,
            ori_model
        ],
        mask=[
            tgt_vectors._keras_mask,
            ori_model._keras_mask
        ]
    )



    transfer_vec, _, _ = self.decoder2(
        self.decoder(
          tgt_model_attention, 
          initial_state=[
              ori_state_h,
              ori_state_c
          ]
        )
    )

    output = self.out(transfer_vec)

    return output, expected, output._keras_mask

def train(epochs, model, batch=64, shuffle=1000, test_size=0.2):

  # Init loss - metrics - optimizers
  loss_function_train = tf.keras.losses.SparseCategoricalCrossentropy(
      from_logits=True,
      reduction=tf.keras.losses.Reduction.NONE
  )

  loss_function_val = tf.keras.losses.SparseCategoricalCrossentropy(
      from_logits=True,
      reduction=tf.keras.losses.Reduction.NONE
  )

  metric_function_train = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')
  metric_function_val = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')

  optimizer = tf.keras.optimizers.RMSprop(
      learning_rate=1e-3
  )

  train_losses = []
  train_accuracy = []
  val_losses = []
  val_accuracy = []

  # Split Dataset
  mid = int(datasets.cardinality().numpy() * (1 - test_size))

  train_ds = datasets.take(mid)
  test_ds = datasets.skip(mid)
  
  # Shuffle Dataset
  train_ds = train_ds.shuffle(shuffle).batch(batch).cache()
  test_ds = datasets.shuffle(shuffle).batch(8).cache()

  for epoch in range(epochs):
    epoch_losses_train = []
    epoch_losses_val = []

    epoch_acc_train = []
    epoch_acc_val = []


    # Training Step
    for step, ((original, target), (original_val, target_val)) in enumerate(zip(train_ds, test_ds)):

      # Training
      with tf.GradientTape() as tape:
        # Foward-pass
        logits, expected, mask = model(original, target, training=True)

        # Compute loss
        loss = loss_function_train(expected, logits)

        # loss = tf.ragged.boolean_mask(loss, mask)
        # loss = tf.reduce_sum(loss) * (1. / batch)

        mask = tf.cast(expected != 0, loss.dtype)
        loss *= mask
        loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)
      
      epoch_losses_train.append(loss.numpy())

      # Retrieve loss
      grads = tape.gradient(loss, model.trainable_weights)

      # Update weight gradient-based
      optimizer.apply_gradients(zip(grads, model.trainable_weights))


      # Compute acc
      logits = tf.argmax(logits, axis=-1)
      logits = tf.cast(logits, expected.dtype)

      match = tf.cast(logits == expected, tf.float32)
      mask = tf.cast(logits != 0, tf.float32)


      metric = tf.reduce_sum(match)/tf.reduce_sum(mask)

      epoch_acc_train.append(
          metric.numpy()
      )

      # metric_function_train.update_state(expected, logits)



      # Validation
      logits, expected, mask = model(original_val, target_val)


      # Compute loss

      loss = loss_function_val(expected, logits)

      # loss = tf.ragged.boolean_mask(loss, val_mask)
      # loss = tf.reduce_sum(loss) * (1. / 8)

      mask = tf.cast(expected != 0, loss.dtype)
      loss *= mask
      loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)

      
      epoch_losses_val.append(loss.numpy())


      # Compute acc
      logits = tf.argmax(logits, axis=-1)
      logits = tf.cast(logits, expected.dtype)

      match = tf.cast(logits == expected, tf.float32)
      mask = tf.cast(logits != 0, tf.float32)

      acc = tf.reduce_sum(match)/tf.reduce_sum(mask)

      epoch_acc_val.append(
          acc.numpy()
      )

      # metric_function_val.update_state(val_expected, val_logits)
    
      if step % 2000 == 0:
        print(f"Step-{step} - loss: {epoch_losses_train[step]} acc: {epoch_acc_train[step]}")


    steps_losses = np.mean(epoch_losses_train)
    steps_acc = np.mean(epoch_acc_train)



    train_losses.append(steps_losses)
    train_accuracy.append(steps_acc)

    val_losses.append(np.mean(epoch_losses_val))
    val_accuracy.append(np.mean(epoch_acc_val))

    # Reset metrics
    # metric_function_train.reset_states()
    # metric_function_val.reset_states()

    print('Trained epoch: {}; loss: {}; accuracy: {}'.format(epoch, steps_losses, steps_acc))

  
  wandb.log({
      "train_loss": train_losses.numpy(),
      "train_accuracy": train_accuracy.numpy(),
      "val_loss": val_losses.numpy(),
      "val_accuracy": val_accuracy.numpy()
  })
  
  plt.plot(train_losses)
  plt.xlabel('Epochs')
  plt.ylabel('Losses')


  tf.saved_model.save(model, 'NMTForStyleTextTransfer')

def translate(original_txt, model, max_seq=100):
    ori_tokens = model.original_process_text([original_txt]) # Shape: (1, Ts)
    ori_vectors = model.ori_embedding(ori_tokens, training=False) # Shape: (1, Ts, embedding_dim)
    ori_model, forward_h, forward_c, backward_h, backward_c = model.encoder(ori_vectors, training=False) # Shape: (batch, rnn_output_dim)
    ori_h_state = tf.concat([forward_h, backward_h], -1)
    ori_c_state = tf.concat([forward_c, backward_c], -1)
    state = [ori_h_state, ori_c_state]
    print(ori_model.shape)

    index_from_string = tf.keras.layers.StringLookup(
        vocabulary=model.original_process_text.get_vocabulary(),
        mask_token='')
    trans = ['[START]']
    vectors = []
    
    for i in range(max_seq):
        token = index_from_string([[trans[i]]]) # Shape: (1, 1)
        vector = model.tgt_embedding(token, training=False) # Shape: (1, 1, embedding_dim)
        vectors.append(vector)
        query = tf.concat(vectors, axis=1)
        context = model.attention(inputs=[query, ori_model], training=False)
        trans_vector, h_state, c_state = model.decoder(context[:,-1:,:], initial_state=state, training=False) # Shape: (1, 1, rnn_output_dim), (1, rnn_output_dim), (1, rnn_output_dim)
        state = [h_state, c_state]
        out = model.out(trans_vector) # Shape: (1, 1, eng_vocab_size)
        out = tf.squeeze(out) # Shape: (eng_vocab_size,)
        word_index = tf.math.argmax(out)
        word = model.original_process_text.get_vocabulary()[word_index]
        trans.append(word)
        if word == '[END]':
            trans = trans[:-1]
            break
    _, atts = model.attention(inputs=[vectors, ori_model], return_attention_scores=True, training=False)
    return ' '.join(trans[1:]), atts

EPOCHS = 70
model = NeuralTextTransfer(raw_text_processor, target_text_processor, 1024)

train(EPOCHS, model, batch=BATCH_SIZE, shuffle=10000)

original_text = [
    'aku gatau kalo bisa kek gitu', 
    'Tapi kami panggil itu dia harus berizin, ',
    'Ini membuat darah tinggi saya, harus memukulnya dengan palu kayu.',
    'Saya, usaha bukan mencari uang, tapi mencari berkah.',
    'Bahkan ujar, produk yang akan, mendapatkan pengawasan dari langsung.',
    'Saat ini, perdana uang -. 000 Rp 20. 000.'
]

predictions = [translate(
    i,
    model, 
    max_seq=100
)[0] for i in original_text]



samples = wandb.Table(columns=["No", "Prompt", "Response"])


for index, (text, prediction) in enumerate(zip(original_text, predictions)):
  samples.add_data(
      index,
      text,
      prediction
  )

wandb.log({"sample_text":samples})

model = tf.keras.models.load_model('/content/NMTForStyleTextTransfer')

model.save('/content/NMTForStyleTextTransfer.h5')

translate(
    'aku gatau kalo bisa kek gitu', 
    model, 
    max_seq=100
)