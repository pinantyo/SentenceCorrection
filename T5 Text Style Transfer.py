# -*- coding: utf-8 -*-
"""T5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yp1a6a19Yx-_AabGlMVMY3xmmNYEyQXv
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %pip install -q transformers sentencepiece simpletransformers
# %pip install --upgrade -q tensorflow

import pandas as pd
import numpy as np

import os, re

import tensorflow as tf
from transformers import T5Tokenizer, TFT5ForConditionalGeneration, TFTrainer, TFTrainingArguments

from sklearn.model_selection import train_test_split

dataset_30 = pd.read_csv(
    "/content/drive/MyDrive/PA_NLP/Torche Intern NLP Progress/Progress/datasets/Indonesian Augmented Dataset-30.csv",
    encoding="utf-8"
)

dataset_50 = pd.read_csv(
    "/content/drive/MyDrive/PA_NLP/Torche Intern NLP Progress/Progress/datasets/Indonesian Augmented Dataset-50.csv",
    encoding="utf-8"
)

dataset_40 = pd.read_csv(
    "/content/drive/MyDrive/PA_NLP/Torche Intern NLP Progress/Progress/datasets/Indonesian Augmented Dataset-40.csv",
    encoding="utf-8"
)

dataset = pd.concat([
    dataset_30, 
    dataset_40, 
    dataset_50],
    ignore_index=True
)

dataset.shape

# Drop Duplication
duplication = dataset.duplicated(subset=['Kalimat']).sum()

print(f'Duplicaton Sum {duplication}')

if duplication:
  dataset.drop_duplicates(
      subset=['Kalimat'],
      inplace=True,
      ignore_index=True,
  )

dataset.shape

# Remove _ values
dataset['Kalimat'] = dataset['Kalimat'].apply(lambda x:re.sub('_','',x))

# X_train, X_test = train_test_split(
#     dataset, 
#     test_size=0.2,
# )

# print(X_train.shape)
# print(X_test.shape)

# # Tokenize the training and validation datasets

# MAX_LENGTH = 64

# train_encodings = tokenizer([pair for pair in X_train['Kalimat']], 
#                             target_text=[pair for pair in X_train['Formal Sentences']],
#                             truncation=True, padding=True, max_length=MAX_LENGTH)

# val_encodings = tokenizer([pair for pair in X_test['Kalimat']],
#                           target_text=[pair for pair in X_test['Formal Sentences']],
#                           truncation=True, padding=True, max_length=MAX_LENGTH)

# # Define the T5 model configuration
# config = {
#     'max_length': MAX_LENGTH,
#     'pad_token_id': tokenizer.pad_token_id,
#     'do_sample': True,
#     'temperature': 0.7,
#     'num_return_sequences': 1
# }

# # Instantiate the T5 model
# model = TFT5ForConditionalGeneration.from_pretrained(model_name, config)

# model.compile(
#     optimizer = tf.keras.optimizers.Adam(1e-4),
#     loss = tf.keras.losses.SparseCategoricalCrossentropy()
# )

# # Define the training parameters
# training_args = TFTrainingArguments(
#     output_dir='./results',
#     num_train_epochs=3,
#     per_device_train_batch_size=2,
#     per_device_eval_batch_size=2,
#     warmup_steps=500,
#     weight_decay=0.01,
#     logging_dir='./logs',
#     logging_steps=100,
#     save_steps=100,
#     evaluation_strategy='steps',
#     eval_steps=100,
#     overwrite_output_dir=True,
#     learning_rate=1e-4,
# )

# # Define the training function
# def train():
#     trainer = TFTrainer(
#         model=model,
#         args=training_args,
#         train_dataset=train_encodings,
#         eval_dataset=val_encodings
#     )
#     trainer.train()

# # Train the model
# train()

# Add Prefix Mode
prefix_df = pd.DataFrame({
    'prefix' : ['informal converter'] * dataset.shape[0]
})

prefix_df.shape

# Reset Index
dataset.reset_index(inplace=True)
new_dataset = pd.merge(prefix_df, dataset, left_index=True, right_index=True)
new_dataset.shape

# Drop Unused Col
new_dataset.drop(columns=['Unnamed: 0', 'index'], inplace=True)

# Format Cols Name
new_dataset.columns = ['prefix', 'input_text', 'target_text']# Drop Duplication

train_df, eval_df = train_test_split(
    new_dataset, test_size=0.2, shuffle=True, random_state=10000
)

print(train_df.shape)
print(eval_df.shape)

eval_df.tail()

eval_df['input_text'].values[-4]

"""## Modelling - T5
---
* https://wandb.ai/yepster/tpu-t5-base/reports/Adafactor-learning-rate-0-005-seems-best-for-t5-base-training--VmlldzoxNTgyODIw
"""

import logging
from simpletransformers.t5 import T5Model, T5Args
logging.basicConfig(level=logging.INFO)
transformers_logger = logging.getLogger("transformers")
transformers_logger.setLevel(logging.WARNING)

model_args = T5Args()

# General Params
model_args.best_model_dir = '/content/drive/MyDrive/PA_NLP/Torche Intern NLP Progress/Progress/checkpoints/T5Best'
model_args.eval_batch_size = 8
model_args.evaluate_generated_text = True
model_args.evaluate_during_training = True
# model_args.evaluate_during_training_steps = 2400
model_args.evaluate_during_training_verbose = True
model_args.early_stopping = True

# model_args.do_lower_case = True
# model_args.dynamic_quantize = True
model_args.early_stopping_consider_epochs = True
model_args.early_stopping_patience = 5
# model_args.save_steps = 2400
model_args.learning_rate = 5e-3 # 5e-4 # 5e-5 # 5e-6 # 5e-7
model_args.max_seq_length = 100
model_args.optimizer = 'Adafactor' # 'AdamW'
model_args.output_dir = '/content/drive/MyDrive/PA_NLP/Torche Intern NLP Progress/Progress/checkpoints/T5'
model_args.overwrite_output_dir = True
model_args.train_batch_size = 32
model_args.use_multiprocessing_for_evaluation = True
model_args.weight_decay = 0.01
model_args.save_eval_checkpoints = True

# WanDB
model_args.wandb_project = "textstyletransfer"


# T5 Params
model_args.num_train_epochs = 200
model_args.do_sample = True
model_args.max_length = 100
model_args.evaluate_generated_text = False

model = T5Model(
    "t5",
    "t5-small",
    # "/content/drive/MyDrive/PA_NLP/Torche Intern NLP Progress/Progress/checkpoints/T5/T5-2",     
    args=model_args,
    use_cuda=True
)

# def count_matches(labels, preds):
#     print(labels)
#     print(preds)
#     return sum([1 if label == pred else 0 for label, pred in zip(labels, preds)])

model.train_model(train_df.applymap(str), eval_data=eval_df.applymap(str))

print(model.eval_model(eval_df))

model = T5Model(
    "t5",
    "/content/drive/MyDrive/PA_NLP/Torche Intern NLP Progress/Progress/checkpoints/T5Best",
    args=model_args,
    use_cuda=True
)

prediction = model.predict(
    [f"informal converter: {eval_df['input_text'].values[-4]}"]
)

print(f"Original Text: {eval_df['input_text'].values[-4]} ")
print(f"Convertion: {prediction}")

text = eval_df['input_text'].values[0]

prediction = model.predict(
    [f"informal converter: {text}"]
)

print(f"Original Text: {text} ")
print(f"Convertion: {prediction}")

eval_df['input_text'].values[2]

text = eval_df['input_text'].values[10]

prediction = model.predict(
    [f"informal converter: {text}"]
)

print(f"Original Text: {text} ")
print(f"Convertion: {prediction}")

text = eval_df['input_text'].values[2]

prediction = model.predict(
    [f"informal converter: {text}"]
)

print(f"Original Text: {text} ")
print(f"Convertion: {prediction}")

"""## Modelling - Seq2Seq"""

from simpletransformers.seq2seq import Seq2SeqModel, Seq2SeqArgs

model_args = Seq2SeqArgs()

# General Params
model_args.best_model_dir = '/content/drive/MyDrive/PA_NLP/Torche Intern NLP Progress/Progress/checkpoints/Seq2SeqBest'
model_args.eval_batch_size = 8
model_args.evaluate_generated_text = True
model_args.evaluate_during_training = True
# model_args.evaluate_during_training_steps = 2400
model_args.evaluate_during_training_verbose = True
model_args.early_stopping = True

# model_args.do_lower_case = True
# model_args.dynamic_quantize = True
# model_args.save_steps = 2400
model_args.learning_rate = 5e-3
model_args.max_seq_length = 100
model_args.optimizer = 'Adafactor' # 'AdamW'
model_args.output_dir = '/content/drive/MyDrive/PA_NLP/Torche Intern NLP Progress/Progress/checkpoints/Seq2Seq'
model_args.overwrite_output_dir = True
model_args.train_batch_size = 16
model_args.use_multiprocessing_for_evaluation = True
model_args.weight_decay = 0.01
model_args.save_eval_checkpoints = True
model_args.num_train_epochs = 200

train_df = train_df.drop(columns=['prefix'])
eval_df = eval_df.drop(columns=['prefix'])

print(train_df.shape)

model = Seq2SeqModel(
    "distilbert",
    "distilbert-base-multilingual-cased",
    "distilbert-base-multilingual-cased",
    args=model_args,
    use_cuda=False,
)

model.train_model(train_df.applymap(str), eval_data=eval_df.applymap(str))

print(model.eval_model(eval_df))