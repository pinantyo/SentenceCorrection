# -*- coding: utf-8 -*-
"""NLP Augmentation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1R9cCjk-HaEUpE2jJdmUFiAoa33BLJyUN
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install -q nlpaug transformers sentencepiece

import pandas as pd
import numpy as np

import os, re

import nlpaug.augmenter.word as naw
import nlpaug.augmenter.sentence as nas

df = pd.read_csv(
    "/content/drive/MyDrive/PA_NLP/Torche Intern NLP Progress/Progress/datasets/Indonesian cola dataset v3.csv",
    encoding="utf-8"
)

df.dropna(inplace=True, subset=['Formal Sentences'])


shorthand_normalization_df = pd.read_csv(
    "/content/drive/MyDrive/PA_NLP/Torche Intern NLP Progress/Progress/datasets/Informal-Formal-Singkatan.csv",
    encoding="utf-8"
)

shorthand_normalization_df = shorthand_normalization_df[['transformed','original-for']]
shorthand_normalization_df.columns = ['Kalimat', 'Formal Sentences']


print(df.shape)
print(shorthand_normalization_df.shape)

df = df[
    ~(df.Check == 'REJECTED')
]
df.shape

"""# Preprocessing"""

def text_normalization(text):
  email_pattern = re.compile(r'[\w._%+-]+@[\w\.-]+\.[a-zA-Z]{2,4}')
  phone_pattern = re.compile(r'(\d{3}[-\.\s]??\d{3}[-\.\s]??\d{4,6}|\(\d{3}\)\s*\d{3}[-\.\s]??\d{4,6}|\d{3,4}[-\.\s]??\d{4,6})')
  url_pattern = re.compile(r'www|http:|https:+[^\s]+[\w]')
  
  text = re.sub(email_pattern, "", text)
  text = re.sub(phone_pattern, "", text)
  text = re.sub(url_pattern, "", text) # Menghapus URL selain https

  text = text.lower().strip()                           # Mengubah teks menjadi lower case
  text = re.sub(r"[-_()\"#/@;:<>{}`+=~|.!?,]", "", text)
  text = re.sub(r'https?://\S+|www\.\S+', "", text)     # Menghapus URL
  text = re.sub(r'[^\w\s,.!?-]', "", text)              # Menghapus diluar exception
  text = text.strip()

  text = text.split(".")
  text = [i.split() for i in text]
  text = [" ".join(i)+'.' for i in text]
  text = [i.strip() for i in text]
  return " ".join(text)

df = df[['Kalimat','Formal Sentences']]
df.shape

df['Kalimat'] = df['Kalimat'].astype('str').apply(text_normalization)
df['Formal Sentences'] = df['Formal Sentences'].astype('str')

df = df.applymap(str)
df.shape

"""# Augmentation"""

import random

def words_deletion(text):
  text = text.strip().split()

  masking_percentage = int(len(text) * 20/100)

  length_text = len(text)

  for i in range(masking_percentage):
    random_index = random.randint(0, length_text-1)
    text.pop(random_index)
    length_text -= 1
  
  return " ".join(text)

probs = [0.4, 0.6, 0.7]

for i in probs:
  # Augmen Synonym
  aug = naw.ContextualWordEmbsAug(model_path='indolem/indobertweet-base-uncased', aug_p=i)
  augmented_text = [aug.augment(text) for text in df['Formal Sentences'].values]
  augmented = pd.DataFrame.from_dict({
      "Kalimat":augmented_text, 
      "Formal Sentences":df['Formal Sentences'].values
  })
  
  df = pd.concat([df, augmented], ignore_index=True)

  # Augmen Spelling
  aug = naw.SpellingAug()
  augmented_text = [aug.augment(text) for text in df['Formal Sentences'].values]
  augmented = pd.DataFrame.from_dict({
      "Kalimat":augmented_text, 
      "Formal Sentences":df['Formal Sentences'].values
  })
  df = pd.concat([df, augmented], ignore_index=True)

  """
    Augmen Random 
  """
  aug = naw.RandomWordAug(aug_p=i)

  # Swapping
  augmented_text = [aug.swap(text) for text in df['Formal Sentences'].values]
  augmented = pd.DataFrame.from_dict({
      "Kalimat":augmented_text, 
      "Formal Sentences":df['Formal Sentences'].values
  })
  df = pd.concat([df, augmented], ignore_index=True)

  # Deletion
  augmented_text = [aug.delete(text) for text in df['Formal Sentences'].values]
  augmented = pd.DataFrame.from_dict({
      "Kalimat":augmented_text, 
      "Formal Sentences":df['Formal Sentences'].values
  })
  df = pd.concat([df, augmented], ignore_index=True)

  # Crop
  augmented_text = [aug.delete(text) for text in df['Formal Sentences'].values]
  augmented = pd.DataFrame.from_dict({
      "Kalimat":augmented_text, 
      "Formal Sentences":df['Formal Sentences'].values
  })
  df = pd.concat([df, augmented], ignore_index=True)

  # Subtitute
  augmented_text = [aug.substitute(text) for text in df['Formal Sentences'].values]
  augmented = pd.DataFrame.from_dict({
      "Kalimat":augmented_text, 
      "Formal Sentences":df['Formal Sentences'].values
  })
  df = pd.concat([df, augmented], ignore_index=True)

  # Add Word Normalization
  df = pd.concat([df, shorthand_normalization_df], ignore_index=True)

  # Augmentasi word masking 
  # augmented_text = df['Formal Sentences'].apply(words_deletion)
  # augmented = pd.DataFrame.from_dict({
  #     "Kalimat":augmented_text, 
  #     "Formal Sentences":df['Formal Sentences'].values
  # })
  
  # df = pd.concat([df, augmented], ignore_index=True)

  df.to_csv(f"/content/drive/MyDrive/PA_NLP/Torche Intern NLP Progress/Progress/datasets/Indonesian Augmented Dataset-{int(i*100)}.csv")
df.tail()